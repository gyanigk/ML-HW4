{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f909b31f-cff7-4602-8dbd-123aeb03468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from mingpt.model import GPT\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f730712f-b889-4099-bb82-f9840a713f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTranscripts = pd.read_csv(\"data/Transcripts.csv\", header=None)\n",
    "dfScores = pd.read_csv(\"data/scores.csv\")\n",
    "dfTotal = dfTranscripts.merge(dfScores, left_on=0, right_on=\"Participant\")\n",
    "dfTotal = dfTotal.drop([0,\"Participant\"], axis=1)\n",
    "dfTotal.columns = [\"Transcript\",\"Performance\", \"Excitedness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d7d1cb-f151-4e35-b909-f0fa42fc9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 774.03M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT.from_pretrained(\"gpt2-large\")\n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f833f87e-5972-40b6-a07b-266db2cdefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 30\n",
    "\n",
    "def prompt(message, new_tokens = 3):\n",
    "    tokenizer = BPETokenizer()\n",
    "    x = tokenizer(message).to(device)\n",
    "    y = model.generate(x, max_new_tokens = new_tokens)\n",
    "    output = tokenizer.decode(y.cpu().squeeze())\n",
    "    return output[len(message):]\n",
    "\n",
    "def prompt_outcome(outcome, training):\n",
    "    training_text = \"The following transcripts are interviews which were then rated on a scale of 1 to 7 based on \" + outcome + \". Below are some examples of interviews and their associated \" + outcome + \":\\n\"\n",
    "    for i in range(0,training):\n",
    "        training_text += \"Interview \" + str(i) + \": <Begin>\" + dfTotal.iloc[i][\"Transcript\"] # adding transcript to prompt\n",
    "        training_text += \"<End>For this interview, on a scale of 1 to 7, the interviewee was given a \" + outcome + \" performance score of [\" + str(dfTotal.iloc[i][outcome]) + \"]\\n\" # adding the score for the specific transcript to the prompt\n",
    "\n",
    "    outputs = []\n",
    "    final_prompts = []\n",
    "    for i in range(training, len(dfTotal)):\n",
    "        question = \"Interview \" + str(i) + \": <Beginning>\" + dfTotal.iloc[i][\"Transcript\"] + \"<End>For this interview, on a scale of 1 to 7, the interviewee was given a \" + outcome + \" score of [\"\n",
    "        \n",
    "        output = prompt(training_text + question)\n",
    "        outputs.append(output) # predictions given by model\n",
    "    \n",
    "        final_prompts.append(question + output) # appending model prediction to prompt to be used at end for explainability\n",
    "    \n",
    "        print(\"Predicted: \" + output)\n",
    "        print(\"Actual: \" + str(dfTotal.iloc[i][outcome]))\n",
    "\n",
    "\n",
    "    explainability_prompt = \"The following transcripts are interviews which were then rated on a scale of 1 to 7 based on \" + outcome + \". Below are the interviews and their associated performances:\\n\"\n",
    "    for i in range(len(final_prompts)):\n",
    "        explainability_prompt += final_prompts[i] + \"\\n\"\n",
    "    explainability_prompt += \"These individuals were rated with these outcomes because they specifically \"\n",
    "    \n",
    "    explainability = prompt(explainability_prompt, 30)\n",
    "    end = len(explainability) if explainability.find(\"<|endoftext|>\") == -1 else explainability.find(\"<|endoftext|>\")\n",
    "    explainability = explainability[:end]\n",
    "    \n",
    "    return outputs, explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1adcf3-6208-4b95-914c-47bf2d2d0bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0] and\n",
      "Actual: 4.438737117\n",
      "Predicted: 0] and\n",
      "Actual: 5.184594229\n",
      "Predicted: 3.86\n",
      "Actual: 5.457670152\n",
      "Predicted: 1] and\n",
      "Actual: 5.394588155\n",
      "Predicted: 0] and\n",
      "Actual: 3.991191068\n",
      "Predicted: 1] and\n",
      "Actual: 5.184765895\n",
      "Predicted: 3.8\n",
      "Actual: 4.885305046\n",
      "Predicted: 1] and\n",
      "Actual: 4.589821648\n",
      "Predicted: 0] and\n",
      "Actual: 6.580970948\n",
      "Predicted: 3.8\n",
      "Actual: 4.199100957\n",
      "Predicted: 3.86\n",
      "Actual: 4.495716785\n",
      "Predicted: 1] and\n",
      "Actual: 5.106512317\n",
      "Predicted: 1] and\n",
      "Actual: 4.865379288\n",
      "Predicted: 3.86\n",
      "Actual: 4.19482841\n",
      "Predicted: 1] to\n",
      "Actual: 4.719197949\n",
      "Predicted: 0] and\n",
      "Actual: 5.083079173\n",
      "Predicted: 1] and\n",
      "Actual: 4.73896974\n",
      "Predicted: 1] and\n",
      "Actual: 5.224229536\n",
      "Predicted: 0] and\n",
      "Actual: 5.517476573\n",
      "Predicted: 0] to\n",
      "Actual: 5.836774498\n",
      "Predicted: 3.8\n",
      "Actual: 3.770640294\n",
      "Predicted: 1] and\n",
      "Actual: 5.799303743\n",
      "Predicted: 3.86\n",
      "Actual: 4.70074076\n",
      "Predicted: 3.86\n",
      "Actual: 4.864385739\n",
      "Predicted: 3.86\n",
      "Actual: 4.727262745\n",
      "Predicted: 3.8\n",
      "Actual: 5.609265776\n",
      "Predicted: 3.86\n",
      "Actual: 5.110903936\n",
      "Predicted: 0] and\n",
      "Actual: 5.895206326\n",
      "Predicted: 3.8\n",
      "Actual: 5.157982324\n",
      "Predicted: 3.86\n",
      "Actual: 4.64231432\n",
      "Predicted: 0] and\n",
      "Actual: 4.826983928\n",
      "Predicted: 3] and\n",
      "Actual: 4.824082363\n",
      "Predicted: xx] and\n",
      "Actual: 4.78938728\n",
      "Predicted: 3.8\n",
      "Actual: 5.496785476\n",
      "Predicted: 3.86\n",
      "Actual: 5.846003278\n",
      "Predicted: 0] and\n",
      "Actual: 5.954056805\n",
      "Predicted: 1] and\n",
      "Actual: 5.319551344\n",
      "Predicted: 0] and\n",
      "Actual: 4.474838517\n",
      "Predicted: 0] to\n",
      "Actual: 4.737142823\n",
      "Predicted: 3.86\n",
      "Actual: 4.951525169\n",
      "Predicted: 3.86\n",
      "Actual: 4.439333068\n",
      "Predicted: 3.86\n",
      "Actual: 5.817165632\n",
      "Predicted: 3.86\n",
      "Actual: 5.138196404\n",
      "Predicted: 3.8\n",
      "Actual: 4.29498918\n",
      "Predicted: 1] and\n",
      "Actual: 5.940625367\n",
      "Predicted: 1] and\n",
      "Actual: 4.260459106\n",
      "Predicted: 1] and\n",
      "Actual: 5.437833076\n",
      "Predicted: 1] and\n",
      "Actual: 5.649863377\n",
      "Predicted: 3.86\n",
      "Actual: 3.996953668\n",
      "Predicted: 3.86\n",
      "Actual: 5.051205708\n",
      "Predicted: 0] and\n",
      "Actual: 4.96436573\n",
      "Predicted: 3.86\n",
      "Actual: 6.151381432\n",
      "Predicted: 1] and\n",
      "Actual: 5.022669773\n",
      "Predicted: 1] and\n",
      "Actual: 4.652766174\n",
      "Predicted: 0] to\n",
      "Actual: 5.293564108\n",
      "Predicted: 1] and\n",
      "Actual: 5.011269327\n",
      "Predicted: 3.86\n",
      "Actual: 3.716695667\n",
      "Predicted: 1] and\n",
      "Actual: 4.783504094\n",
      "Predicted: 0] and\n",
      "Actual: 5.386369273\n",
      "Predicted: 0] and\n",
      "Actual: 6.133183139\n",
      "Predicted: 5:41\n",
      "Actual: 5.38249587\n",
      "Predicted: 3.86\n",
      "Actual: 5.399636573\n",
      "Predicted: 3.8\n",
      "Actual: 5.709427944\n",
      "Predicted: 1] and\n",
      "Actual: 5.3558692\n",
      "Predicted: 1] and\n",
      "Actual: 5.762105433\n",
      "Predicted: 3.86\n",
      "Actual: 5.313401184\n",
      "Predicted: 0] and\n",
      "Actual: 5.0608051\n",
      "Predicted: 3.86\n",
      "Actual: 5.538978871\n",
      "Predicted: 3.86\n",
      "Actual: 4.504225692\n",
      "Predicted: 3.86\n",
      "Actual: 3.888033248\n",
      "Predicted: 1] and\n",
      "Actual: 5.109073517\n",
      "Predicted: 3.86\n",
      "Actual: 5.530993449\n",
      "Predicted: 1] and\n",
      "Actual: 5.770041407\n",
      "Predicted: 1] and\n",
      "Actual: 4.057951826\n",
      "Predicted: 1] and\n",
      "Actual: 5.39482101\n",
      "Predicted: 3.8\n",
      "Actual: 5.415966252\n",
      "Predicted: 3.86\n",
      "Actual: 4.437833076\n",
      "Predicted: 1] and\n",
      "Actual: 6.559090569\n",
      "Predicted: 3.8\n",
      "Actual: 5.20562296\n",
      "Predicted: 1] and\n",
      "Actual: 5.574491394\n",
      "Predicted: 1] and\n",
      "Actual: 4.139804595\n",
      "Predicted: 1] and\n",
      "Actual: 5.237471595\n",
      "Predicted: 3.86\n",
      "Actual: 4.872901662\n",
      "Predicted: 1] and\n",
      "Actual: 4.751991269\n",
      "Predicted: 1] and\n",
      "Actual: 4.966807857\n",
      "Predicted: 1] and\n",
      "Actual: 5.768810907\n",
      "Predicted: 1] and\n",
      "Actual: 5.539601809\n",
      "Predicted: 0] and\n",
      "Actual: 6.178052254\n",
      "Predicted: 0] and\n",
      "Actual: 5.660889067\n",
      "Predicted: 0] and\n",
      "Actual: 3.608315215\n",
      "Predicted: 3.86\n",
      "Actual: 5.608698138\n",
      "Predicted: 3.86\n",
      "Actual: 4.760001975\n",
      "Predicted: 3.8\n",
      "Actual: 5.364716965\n",
      "Predicted: 3.86\n",
      "Actual: 5.175140063\n",
      "Predicted: 3.8\n",
      "Actual: 5.647452246\n",
      "Predicted: 3.86\n",
      "Actual: 4.58163265\n",
      "Predicted: 3.8\n",
      "Actual: 5.835438669\n",
      "Predicted: ?]ISaw\n",
      "Actual: 5.092022057\n",
      "Predicted: 3.86\n",
      "Actual: 4.776903203\n",
      "Predicted: 1] and\n",
      "Actual: 5.380115336\n",
      "Predicted: 3.86\n",
      "Actual: 5.31921494\n",
      "Predicted: 1] and\n",
      "Actual: 5.784742242\n",
      "Predicted: ??] and\n",
      "Actual: 6.452418644\n",
      "Predicted: 3.86\n",
      "Actual: 6.045748435\n",
      "Predicted: 0] and\n",
      "Actual: 5.710073178\n",
      "Predicted: 1] and\n",
      "Actual: 5.62607415\n",
      "Predicted: 1] and\n",
      "Actual: 4.853881081\n",
      "Predicted: ???] and\n",
      "Actual: 4.96008413\n",
      "Predicted: 1] and\n",
      "Actual: 3.790904315\n",
      "Predicted: 1] and\n",
      "Actual: 5.433575118\n",
      "Predicted: 3.17\n",
      "Actual: 4.707062188\n",
      "Predicted: 5] and\n",
      "Actual: 3.994165679\n",
      "Predicted: 1] and\n",
      "Actual: 3.484147432\n",
      "Predicted: ??] and\n",
      "Actual: 4.838372063\n",
      "Predicted: 3.17\n",
      "Actual: 4.318191448\n",
      "Predicted: 1] and\n",
      "Actual: 3.651651546\n",
      "Predicted: 1] and\n",
      "Actual: 6.140326849\n",
      "Predicted: 3.17\n",
      "Actual: 3.527104177\n",
      "Predicted: 3.17\n",
      "Actual: 5.15663294\n",
      "Predicted: 1] and\n",
      "Actual: 4.648259352\n",
      "Predicted: 1] and\n",
      "Actual: 5.419990124\n",
      "Predicted: 3.17\n",
      "Actual: 3.045424429\n",
      "Predicted: 1] and\n",
      "Actual: 4.666731717\n",
      "Predicted: 0] and\n",
      "Actual: 3.909424899\n",
      "Predicted: 1] and\n",
      "Actual: 4.388821458\n",
      "Predicted: 1] and\n",
      "Actual: 4.53060124\n",
      "Predicted: 0] and\n",
      "Actual: 6.214156589\n",
      "Predicted: 1] and\n",
      "Actual: 5.736839296\n",
      "Predicted: 3.17\n",
      "Actual: 3.125481338\n",
      "Predicted: 1] and\n",
      "Actual: 4.886049732\n",
      "Predicted: 3.17\n",
      "Actual: 5.266866197\n",
      "Predicted: 3.17\n",
      "Actual: 4.513576221\n",
      "Predicted: 3.17\n",
      "Actual: 4.035367296\n",
      "Predicted: 3.17\n",
      "Actual: 5.026200758\n",
      "Predicted: 3.17\n",
      "Actual: 4.571966282\n",
      "Predicted: 0] and\n",
      "Actual: 4.95210301\n",
      "Predicted: 3.17\n",
      "Actual: 5.068456484\n",
      "Predicted: 3.17\n",
      "Actual: 4.874057495\n",
      "Predicted: 1] and\n",
      "Actual: 5.504738659\n",
      "Predicted: 1] and\n",
      "Actual: 4.2339731\n",
      "Predicted: xx] and\n",
      "Actual: 5.287228548\n",
      "Predicted: 3.17\n",
      "Actual: 4.90786787\n",
      "Predicted: 3.17\n",
      "Actual: 6.072378039\n",
      "Predicted: 0] and\n",
      "Actual: 5.748063915\n",
      "Predicted: ???] and\n",
      "Actual: 5.159087391\n",
      "Predicted: 0.5\n",
      "Actual: 5.245840676\n",
      "Predicted: 0.5\n",
      "Actual: 3.85341414\n",
      "Predicted: 3.17\n",
      "Actual: 4.223099473\n",
      "Predicted: 3.17\n",
      "Actual: 4.866626895\n",
      "Predicted: 3.17\n",
      "Actual: 4.815860401\n",
      "Predicted: 3.17\n",
      "Actual: 5.046974839\n",
      "Predicted: 3.17\n",
      "Actual: 3.415120863\n",
      "Predicted: 1] and\n",
      "Actual: 5.877261452\n",
      "Predicted: 1] and\n",
      "Actual: 4.134752561\n",
      "Predicted: 5] and\n",
      "Actual: 4.838466127\n",
      "Predicted: 1] and\n",
      "Actual: 5.122305605\n",
      "Predicted: 3.17\n",
      "Actual: 3.697667988\n",
      "Predicted: 3.17\n",
      "Actual: 5.019087645\n",
      "Predicted: 1] and\n",
      "Actual: 4.494770295\n",
      "Predicted: 3.17\n",
      "Actual: 5.68979625\n",
      "Predicted: 1] and\n",
      "Actual: 5.078528199\n",
      "Predicted: 1] and\n",
      "Actual: 4.254742342\n",
      "Predicted: 0.5\n",
      "Actual: 4.84387699\n",
      "Predicted: 1] and\n",
      "Actual: 5.8943886\n",
      "Predicted: 3.17\n",
      "Actual: 3.82418988\n",
      "Predicted: 1] and\n",
      "Actual: 5.49574142\n",
      "Predicted: 0] and\n",
      "Actual: 6.1089171\n",
      "Predicted: 1] and\n",
      "Actual: 6.447926587\n",
      "Predicted: 5:41\n",
      "Actual: 5.592839162\n",
      "Predicted: 3.17\n",
      "Actual: 5.354908128\n",
      "Predicted: 3.17\n",
      "Actual: 5.414533884\n",
      "Predicted: 1] and\n",
      "Actual: 5.563943253\n",
      "Predicted: 1] and\n",
      "Actual: 5.201322553\n",
      "Predicted: 3.17\n",
      "Actual: 5.001892202\n",
      "Predicted: 0] and\n",
      "Actual: 3.907237225\n",
      "Predicted: 3.17\n",
      "Actual: 5.251523706\n",
      "Predicted: 3.17\n",
      "Actual: 3.683352027\n",
      "Predicted: 3.17\n",
      "Actual: 2.947854996\n",
      "Predicted: 5] and\n",
      "Actual: 4.833639306\n",
      "Predicted: 3.17\n",
      "Actual: 4.88960961\n",
      "Predicted: 1] and\n",
      "Actual: 4.849643687\n",
      "Predicted: 1] and\n",
      "Actual: 3.307912725\n",
      "Predicted: 1] and\n",
      "Actual: 5.599982872\n",
      "Predicted: 3.17\n",
      "Actual: 4.795140676\n",
      "Predicted: 3.17\n",
      "Actual: 4.222688326\n",
      "Predicted: 1] and\n",
      "Actual: 6.273069487\n",
      "Predicted: 3.17\n",
      "Actual: 4.620665741\n",
      "Predicted: 1] and\n",
      "Actual: 6.523917364\n",
      "Predicted: 1] and\n",
      "Actual: 3.388856611\n",
      "Predicted: 1] and\n",
      "Actual: 5.601585674\n",
      "Predicted: 3.17\n",
      "Actual: 4.373468677\n",
      "Predicted: 1] and\n",
      "Actual: 4.38964519\n",
      "Predicted: 1] and\n",
      "Actual: 4.187086944\n",
      "Predicted: 1] and\n",
      "Actual: 5.451477543\n",
      "Predicted: 5] and\n",
      "Actual: 4.793210586\n",
      "Predicted: 5] and\n",
      "Actual: 5.900285341\n",
      "Predicted: 0.5\n",
      "Actual: 5.599494539\n",
      "Predicted: 0.5\n",
      "Actual: 2.837465655\n",
      "Predicted: 3.17\n",
      "Actual: 5.257184469\n",
      "Predicted: 3.17\n",
      "Actual: 5.819761303\n",
      "Predicted: 3.17\n",
      "Actual: 5.012913897\n",
      "Predicted: 3.17\n",
      "Actual: 5.408563226\n",
      "Predicted: 3.17\n",
      "Actual: 5.338638415\n",
      "Predicted: 3.17\n",
      "Actual: 4.622793732\n",
      "Predicted: 3.17\n",
      "Actual: 4.804776446\n",
      "Predicted: ?] and a\n",
      "Actual: 5.014643892\n",
      "Predicted: 3.17\n",
      "Actual: 4.804416284\n",
      "Predicted: 1] and\n",
      "Actual: 6.435390949\n",
      "Predicted: 3.17\n",
      "Actual: 4.143843056\n",
      "Predicted: 1] and\n",
      "Actual: 4.954331426\n",
      "Predicted: ??] and\n",
      "Actual: 5.791487128\n",
      "Predicted: 3.17\n",
      "Actual: 5.806617492\n",
      "Predicted: 1] and\n",
      "Actual: 5.307262103\n",
      "Predicted: 1] and\n",
      "Actual: 5.584830083\n",
      "Predicted: 1] and\n",
      "Actual: 5.137644188\n",
      "Predicted: ???] and\n",
      "Actual: 4.407999281\n"
     ]
    }
   ],
   "source": [
    "performances, performances_explainability = prompt_outcome(\"Performance\", training)\n",
    "excitedness, excitedness_explainability = prompt_outcome(\"Excitedness\", training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc12526d-fc66-4759-8c0f-ed3cf251903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3.8', '3.86', '1] and', '0] and', '5:41', '??] and', '???] and', '0] to', '?]ISaw', '1] to', 'xx] and', '3] and'}\n",
      "{'??] and', '1] and', '0] and', '5:41', '???] and', '5] and', 'xx] and', '0.5', '?] and a', '3.17'}\n",
      "['0.5' '0] and' '1] and' '3.17' '5:41' '5] and' '???] and' '??] and'\n",
      " '?] and a' 'xx] and']\n",
      "[ 5  6 40 45  1  5  2  2  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(set(performances))\n",
    "print(set(excitedness))\n",
    "values, counts = np.unique(np.array(excitedness), return_counts = True)\n",
    "print(values)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5469388e-6cc1-411f-bde8-0b5513698c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = [3.8 if element == '3.8' else element for element in performances]\n",
    "performances = [3.86 if element == '3.86' else element for element in performances]\n",
    "performances = [1 if element == '1] and' else element for element in performances]\n",
    "performances = [1 if element == '1] to' else element for element in performances]\n",
    "performances = [1 if element == '1' else element for element in performances]\n",
    "performances = [0 if element == '0] and' else element for element in performances]\n",
    "performances = [0 if element == '0] to' else element for element in performances]\n",
    "performances = [0 if element == '0' else element for element in performances]\n",
    "performances = [5 if element == '5:41' else element for element in performances]\n",
    "performances = [5 if element == '5' else element for element in performances]\n",
    "performances = [3 if element == '3] and' else element for element in performances]\n",
    "performances = [3 if element == '3' else element for element in performances]\n",
    "\n",
    "performances_boolean = [not isinstance(element, str) for element in performances]\n",
    "\n",
    "\n",
    "excitedness = [0.5 if element == '0.5' else element for element in excitedness]\n",
    "excitedness = [0 if element == '0] and' else element for element in excitedness]\n",
    "excitedness = [0 if element == '0' else element for element in excitedness]\n",
    "excitedness = [1 if element == '1] and' else element for element in excitedness]\n",
    "excitedness = [1 if element == '1' else element for element in excitedness]\n",
    "excitedness = [3.17 if element == '3.17' else element for element in excitedness]\n",
    "excitedness = [5 if element == '5:41' else element for element in excitedness]\n",
    "excitedness = [5 if element == '5] and' else element for element in excitedness]\n",
    "excitedness = [5 if element == '5' else element for element in excitedness]\n",
    "\n",
    "excitedness_boolean = [not isinstance(element, str) for element in excitedness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bda84fdf-39fb-45dc-b7de-79402142b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '3' '3.8' '3.86' '5' '???] and' '??] and' '?]ISaw' 'xx] and']\n",
      "[21 36  1 13 32  1  1  1  1  1]\n",
      "['0' '0.5' '1' '3.17' '5' '???] and' '??] and' '?] and a' 'xx] and']\n",
      "[ 6  5 40 45  6  2  2  1  1]\n"
     ]
    }
   ],
   "source": [
    "values, counts = np.unique(np.array(performances), return_counts = True)\n",
    "print(values)\n",
    "print(counts)\n",
    "\n",
    "values, counts = np.unique(np.array(excitedness), return_counts = True)\n",
    "print(values)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a782088c-47e1-419d-906a-e01995fb8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall r: -0.16522894471972746\n",
      "Overall re: 0.5818033715139447\n",
      "Excited r: -0.1241242463564663\n",
      "Excited re: 0.5554049180741379\n"
     ]
    }
   ],
   "source": [
    "performances = np.array(performances)[performances_boolean].astype(float)\n",
    "excitedness = np.array(excitedness)[excitedness_boolean].astype(float)\n",
    "\n",
    "r_overall, _ = pearsonr(performances, dfTotal.loc[training:,\"Performance\"][performances_boolean])\n",
    "print(\"Overall r: \" + str(r_overall))\n",
    "\n",
    "re_overall = np.mean(np.abs((dfTotal.loc[training:,\"Performance\"][performances_boolean] - performances) / dfTotal.loc[training:,\"Performance\"][performances_boolean]))\n",
    "print(\"Overall re: \" + str(re_overall))\n",
    "\n",
    "r_overall, _ = pearsonr(excitedness, dfTotal.loc[training:,\"Excitedness\"][excitedness_boolean])\n",
    "print(\"Excited r: \" + str(r_overall))\n",
    "\n",
    "re_overall = np.mean(np.abs((dfTotal.loc[training:,\"Excitedness\"][excitedness_boolean] - excitedness) / dfTotal.loc[training:,\"Excitedness\"][excitedness_boolean]))\n",
    "print(\"Excited re: \" + str(re_overall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919501e-cdd9-44d2-9321-d03e98838007",
   "metadata": {},
   "source": [
    "In terms of performance, this model performed much worse for both overall and excited compared to the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "11eb09af-ae95-471c-bffc-664e9b97676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability of overall perfomance:  attended the interview.\n",
      "Interviewee: [???]\n",
      "Interviewee: [???]\n",
      "Interviewee: [???]\n",
      "Interviewee\n",
      "Explainability of excitedness:  attended the interview.\n"
     ]
    }
   ],
   "source": [
    "print(\"Explainability of overall perfomance: \" + performances_explainability)\n",
    "print(\"Explainability of excitedness: \" + excitedness_explainability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364aa76-1cc6-482f-b77f-96a3e54e25c2",
   "metadata": {},
   "source": [
    "When the model was asked why it came up with all of its decisions, it was not good at explaining as you can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a968ffe3-ed17-4f94-bd00-81d397447c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0, Interview 30:  he was not able to perform the tasks assigned to him.\n",
      "Score: 0.0, Interview 31:  he did not perform as expected. The interviewee was given a performance score of [1.0] because he performed as expected. The interviewee\n",
      "Score: 3.86, Interview 32:  he was able to demonstrate leadership and leadership skills.\n",
      "Score: 1.0, Interview 33: __________.\n",
      "Score: 0.0, Interview 34:  he did not perform to the expectations of the interviewer.\n"
     ]
    }
   ],
   "source": [
    "# overall\n",
    "for i in range(training, training+5):\n",
    "    prompt_text = \"Interview \" + str(i) + \": <Beginning>\" + dfTotal.iloc[i][\"Transcript\"] + \"<End>For this interview, on a scale of 1 to 7, the interviewee was given a performance score of [\" + str(performances[i-training]) + \"] because \"\n",
    "    output = prompt(prompt_text, 30)\n",
    "    end = len(output) if output.find(\"<|endoftext|>\") == -1 else output.find(\"<|endoftext|>\")\n",
    "    print(\"Score: \" + str(performances[i-training]) + \", Interview \" + str(i) + \": \" + output[:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3812afd-f30f-4760-aaee-60337f3485f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0, Interview 30:  he was excited to be working on a project. The interviewer was given a neutral score of [0.0] because the interviewer was not excited to\n",
      "Score: 1.0, Interview 31:  he was excited about the opportunity to work with the interviewer. The interviewer was given a nervousness score of [2.0] because  he was\n",
      "Score: 3.17, Interview 32:  he was excited about the opportunity to work with the interviewer and the interviewer was excited about the opportunity to work with the interviewee.\n",
      "Score: 5.0, Interview 33: __________ (the interviewer) was very interested in the interviewee's background and interests. The interviewer was also very interested in the interviewee's ability\n",
      "Score: 1.0, Interview 34:  he was excited about the job. The interviewer was given a neutral score of [0.0] because  he was neutral about the job. The\n"
     ]
    }
   ],
   "source": [
    "# excited\n",
    "for i in range(training, training+5):\n",
    "    prompt_text = \"Interview \" + str(i) + \": <Beginning>\" + dfTotal.iloc[i][\"Transcript\"] + \"<End>For this interview, on a scale of 1 to 7, the interviewee was given a excitedness score of [\" + str(excitedness[i-training]) + \"] because \"\n",
    "    output = prompt(prompt_text, 30)\n",
    "    end = len(output) if output.find(\"<|endoftext|>\") == -1 else output.find(\"<|endoftext|>\")\n",
    "    print(\"Score: \" + str(excitedness[i-training]) + \", Interview \" + str(i) + \": \" + output[:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119bb64-374c-42c2-934e-b412e6aedeac",
   "metadata": {},
   "source": [
    "When the model was asked to give individual explanations, it focused more on the content of the message and what the individual is conveying rather than the words, etc. that the SHAP model showed. So individuals who showed good qualities (through what they said) had higher outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
