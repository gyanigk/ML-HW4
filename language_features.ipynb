{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loading the dataset and parsing the required information from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "prosodic_file = \"data/prosodic_features.csv\"\n",
    "scores_file = \"data/scores.csv\"\n",
    "transcript_file = \"data/transcripts.csv\"\n",
    "\n",
    "# Read csvs into a file\n",
    "prosodic_data = pd.read_csv(prosodic_file)\n",
    "scores_data = pd.read_csv(scores_file)\n",
    "transcript_data = pd.read_csv(transcript_file)\n",
    "\n",
    "# Add a column that corresponds to participant for easy splitting for prosodic data\n",
    "prosodic_data['Participant'] = prosodic_data['participant&question'].str.extract(r'^(PP?\\d+)')\n",
    "prosodic_data['Participant'] = prosodic_data['Participant'].str.lower()\n",
    "# Make the entire transcript lower case\n",
    "transcript_data['transcript'] = transcript_data['transcript'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of     participant&question    duration    energy  min_pitch   max_pitch  \\\n",
      "0                   P1Q1   51.952125  0.015331  75.232657  396.635613   \n",
      "1                   P1Q2   38.677312  0.015185  75.165527  397.613041   \n",
      "2                   P1Q3   43.593896  0.014680  71.034761  395.930688   \n",
      "3                   P1Q4   23.435813  0.008920  74.938673  248.733738   \n",
      "4                   P1Q5   13.274833  0.003432  93.949854  263.669188   \n",
      "..                   ...         ...       ...        ...         ...   \n",
      "685               PP89Q1   76.333333  0.018540  74.344414  382.978887   \n",
      "686               PP89Q2   60.628396  0.017723  72.955935  396.654629   \n",
      "687               PP89Q3  131.118042  0.028155  72.806624  293.105036   \n",
      "688               PP89Q4   85.464125  0.027269  65.836502  387.213912   \n",
      "689               PP89Q5   49.306229  0.007684  77.520809  375.522678   \n",
      "\n",
      "     mean_pitch   pitch_sd   pitch_abs  pitch_quant  pitchUvsVRatio  ...  \\\n",
      "0    127.989222  27.821528  217.628692   119.690367        0.773830  ...   \n",
      "1    131.067490  26.452853  195.852246   124.820583        0.717333  ...   \n",
      "2    127.739086  26.006330  189.441737   120.457848        1.068376  ...   \n",
      "3    129.563914  19.334327  103.024640   124.473851        1.121487  ...   \n",
      "4    130.178482  26.595483  174.934601   120.272480        1.078493  ...   \n",
      "..          ...        ...         ...          ...             ...  ...   \n",
      "685  142.627278  24.480275  146.910240   136.814271        0.526916  ...   \n",
      "686  145.442889  31.189455  173.895102   135.946832        0.495680  ...   \n",
      "687  138.740341  21.224634  143.577876   132.138036        0.616276  ...   \n",
      "688  142.979222  32.409678  169.441817   132.272303        1.035502  ...   \n",
      "689  135.363581  23.088338  131.030416   128.851294        0.738532  ...   \n",
      "\n",
      "     PercentBreaks  speakRate  numPause  maxDurPause  avgDurPause  \\\n",
      "0          0.42117   0.000342        55     1.290667        0.494   \n",
      "1          0.38674   0.000444        40     1.866667        0.522   \n",
      "2          0.43099   0.000475        44     2.624000        0.505   \n",
      "3          0.46003   0.000907        24     2.933333        0.507   \n",
      "4          0.46576   0.001570        12     2.848000        0.628   \n",
      "..             ...        ...       ...          ...          ...   \n",
      "685        0.34761   0.000200        47     3.861333        0.956   \n",
      "686        0.31794   0.000247        34     4.565333        1.099   \n",
      "687        0.38598   0.000123        96     5.781333        0.889   \n",
      "688        0.50645   0.000238        48     6.240000        1.204   \n",
      "689        0.42317   0.000353        30     5.109333        1.181   \n",
      "\n",
      "     TotDurPause:3  iInterval  MaxRising:3  MaxFalling:3  Participant  \n",
      "0           27.176        138      274.562       257.247           p1  \n",
      "1           20.875        100      268.613       270.193           p1  \n",
      "2           22.229        114      283.912       203.506           p1  \n",
      "3           12.171         55      100.655        83.172           p1  \n",
      "4            7.541         32      129.591       117.119           p1  \n",
      "..             ...        ...          ...           ...          ...  \n",
      "685         44.917        140      163.554       186.015         pp89  \n",
      "686         37.376        126      262.512       255.665         pp89  \n",
      "687         85.330        295      112.819       148.744         pp89  \n",
      "688         57.799        175      250.261       245.283         pp89  \n",
      "689         35.421         90      158.919       253.517         pp89  \n",
      "\n",
      "[690 rows x 37 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(prosodic_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: ['72', '56', '33', '27', '64', '6', '85', '57', '3', '14', '1', '63', '42', '25']\n",
      "\tLength:  14\n",
      "Fold 2: ['30', '49', '34', '24', '29', '55', '16', '17', '61', '45', '74', '89', '37', '69']\n",
      "\tLength:  14\n",
      "Fold 3: ['66', '83', '7', '77', '20', '35', '52', '84', '11', '47', '13', '73', '8', '15']\n",
      "\tLength:  14\n",
      "Fold 4: ['22', '86', '81', '32', '43', '12', '78', '58', '71', '31', '59', '5', '65', '67']\n",
      "\tLength:  14\n",
      "Fold 5: ['21', '62', '4', '50', '70', '79', '53', '44', '80', '60', '10', '76', '48']\n",
      "\tLength:  13\n"
     ]
    }
   ],
   "source": [
    "# Splitting the participants into 5 folds\n",
    "num_folds = 5\n",
    "\n",
    "# Grab participant numbers from the scores csv file\n",
    "interviews = scores_data['Participant'].unique()\n",
    "participants = list(set([re.sub(r'^pp?|q\\d+', '', item) for item in interviews]))\n",
    "random.shuffle(participants)\n",
    "participant_folds = [participants[i::num_folds] for i in range(num_folds)]\n",
    "\n",
    "for i, fold in enumerate(participant_folds):\n",
    "  print(f\"Fold {i + 1}: {fold}\")\n",
    "  print(\"\\tLength: \", len(fold))\n",
    "\n",
    "# Grab all the correct interview names associated with each participant\n",
    "interview_folds = []\n",
    "for fold in participant_folds:\n",
    "  interview_folds.append([item for num in fold for item in (f\"p{num}\", f\"pp{num}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prosodic Data: \n",
      "(410, 37)   (140, 37)   (140, 37)\n",
      "Scores Data: \n",
      "(82, 3)   (28, 3)   (28, 3)\n",
      "Transcript Data: \n",
      "(82, 4)   (28, 4)   (28, 4)\n"
     ]
    }
   ],
   "source": [
    "def get_data_splits(data, fold_number):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets based on a specified fold.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The complete dataset.\n",
    "        fold_number (int): The fold to use for testing (0-based index).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_set, validation_set, testing_set)\n",
    "    \"\"\"\n",
    "    # Quick check on fold number\n",
    "    assert 0 <= fold_number < len(interview_folds), \"Fold_number must be between 0 and len(folds) - 1\"\n",
    "\n",
    "    # Split the data\n",
    "    test_set = data[data['Participant'].isin( interview_folds[fold_number] )]\n",
    "    val_set = data[data['Participant'].isin( interview_folds[(fold_number + 1) % len(interview_folds)] )]\n",
    "    train_set_parts = [item for i, fold in enumerate(interview_folds) if i not in [fold_number, (fold_number + 1) % len(interview_folds)] for item in fold]\n",
    "    train_set = data[data['Participant'].isin(train_set_parts)]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = get_data_splits(prosodic_data, 0)\n",
    "print(\"Prosodic Data: \")\n",
    "print(train_set.shape, \" \", val_set.shape, \" \", test_set.shape)\n",
    "train_set, val_set, test_set = get_data_splits(scores_data, 0)\n",
    "print(\"Scores Data: \")\n",
    "print(train_set.shape, \" \", val_set.shape, \" \", test_set.shape)\n",
    "# print(test_set.head())\n",
    "train_set, val_set, test_set = get_data_splits(transcript_data, 0)\n",
    "print(\"Transcript Data: \")\n",
    "print(train_set.shape, \" \", val_set.shape, \" \", test_set.shape)\n",
    "# print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting language features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gyani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gyani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\gyani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\gyani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')  # POS Tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon') # Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic Vectorization with CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yup</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  100  13  14  15  16  18  20  200  ...  yep  yes  yo  yoga  york  \\\n",
       "0    1   0    0   0   0   0   0   0   0    0  ...    0    1   0     0     0   \n",
       "1    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "2    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "3    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "4    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "\n",
       "   young  younger  youngest  yup  zone  \n",
       "0      0        0         0    0     0  \n",
       "1      0        0         0    0     0  \n",
       "2      0        0         0    0     0  \n",
       "3      0        0         0    0     0  \n",
       "4      0        0         0    0     0  \n",
       "\n",
       "[5 rows x 2464 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Syntactic vectorizer: CountVectorizer\n",
    "\n",
    "# Remove common stop words in english and ignore words that appear fewer than 2 times\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2) \n",
    "X = vectorizer.fit_transform(transcript_data['transcript'])\n",
    "\n",
    "# Convert from sparse X matrix to a denser one for easy use\n",
    "X_dense = X.toarray()\n",
    "feature_names_count = vectorizer.get_feature_names_out()\n",
    "\n",
    "word_count = pd.DataFrame(X_dense, columns=feature_names_count)\n",
    "\n",
    "word_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic Vectorization with TFIDF Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yup</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000   10  100   13   14   15   16   18   20  200  ...  yep       yes  \\\n",
       "0  0.068513  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.031465   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "\n",
       "    yo  yoga  york  young  younger  youngest  yup  zone  \n",
       "0  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "1  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "2  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "3  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "4  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 2464 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Syntactic vectorizer: TFIDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "tfidf_matrix = tfidf.fit_transform(transcript_data['transcript'])\n",
    "feature_names_tf = tfidf.get_feature_names_out()\n",
    "tfidf_count = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names_tf)\n",
    "\n",
    "tfidf_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Features (word count and average word length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word count</th>\n",
       "      <th>avg word length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>613</td>\n",
       "      <td>4.438825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1118</td>\n",
       "      <td>4.510733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751</td>\n",
       "      <td>4.528628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717</td>\n",
       "      <td>4.281729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>645</td>\n",
       "      <td>4.688372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word count  avg word length\n",
       "0         613         4.438825\n",
       "1        1118         4.510733\n",
       "2         751         4.528628\n",
       "3         717         4.281729\n",
       "4         645         4.688372"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Statistical Features (word count and average word length)\n",
    "\n",
    "# Word Count for the entire interview\n",
    "features = pd.DataFrame()\n",
    "features['word count'] = transcript_data['transcript'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Average word length for the interview\n",
    "features['avg word length'] = transcript_data['transcript'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gyani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "1    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "2    [(interviewer, NN), (:, :), (so, RB), (tell, V...\n",
       "3    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "4    [(interviewer, NN), (:, :), (how, WRB), (are, ...\n",
       "Name: pos tagging, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['pos tagging'] = transcript_data['transcript'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
    "features['pos tagging'].head()\n",
    "# NN: Noun singular\n",
    "# VB: Verb base form\n",
    "# JJ: Adjective\n",
    "# RB: Adverb\n",
    "# DT: Determiner\n",
    "# IN: Preposition or subordination conjunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis with Vader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'neg': 0.013, 'neu': 0.859, 'pos': 0.128, 'co...\n",
       "1    {'neg': 0.026, 'neu': 0.852, 'pos': 0.122, 'co...\n",
       "2    {'neg': 0.02, 'neu': 0.874, 'pos': 0.105, 'com...\n",
       "3    {'neg': 0.027, 'neu': 0.842, 'pos': 0.131, 'co...\n",
       "4    {'neg': 0.038, 'neu': 0.838, 'pos': 0.124, 'co...\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis with Vader\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "features['sentiment'] = transcript_data['transcript'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "\n",
    "features['sentiment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings with BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322d69dc01b345adb56aa1287f85e48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gyani\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59494c432b944299841ee17d6b7a1fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16df99580b9649f3bea959368d32d2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e7bc5d0c3547cfb32d4159d789fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249543b2741c4ad899055822524a9cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Word embeddings with BERT (Hugging Face)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.20702128, -0.34244847, 0.10633583, 0.048458...\n",
       "1    [0.20338869, -0.42223406, -0.0042748363, -0.00...\n",
       "2    [0.3116174, -0.17067035, -0.094398685, 0.18208...\n",
       "3    [0.21430025, -0.47013944, -0.07054973, 0.12508...\n",
       "4    [0.17230605, -0.37611702, -0.10042618, -0.0261...\n",
       "Name: word embeddings, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate interveiw into sentences\n",
    "def process_transcript(text):\n",
    "  sentences = re.split(r'(?<=\\|)', text)\n",
    "  cleaned_sentences = [\n",
    "    re.sub(r'^(interviewer:|interviewee:)\\s*', '', s.strip().replace('|', '')) for s in sentences if s.strip()\n",
    "  ]\n",
    "  return cleaned_sentences\n",
    "\n",
    "transcript_data['Processed Transcript'] = transcript_data['transcript'].apply(process_transcript)\n",
    "\n",
    "# Tokenize the text\n",
    "def get_bert_embeddings(sentences):\n",
    "  embeddings = []\n",
    "  for sentence in sentences:\n",
    "    # Tokenize and get input Id and attention mask\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=20)\n",
    "\n",
    "    # Use BERT\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "\n",
    "    # Pool output\n",
    "    cls_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    embeddings.append(cls_embedding)\n",
    "\n",
    "  # Return an array that is the same size\n",
    "  return np.array(embeddings).flatten()[:6912]\n",
    "\n",
    "features['word embeddings'] = transcript_data['Processed Transcript'].apply(get_bert_embeddings)\n",
    "\n",
    "features['word embeddings'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Features Extracted:\n",
    "- Syntactic Vectorization with CountVectorizer\n",
    "- Syntactic Vectorization with TFIDF Vectorizer\n",
    "- Word Count\n",
    "- Average Word Length\n",
    "- Part of Speech Tagging\n",
    "- Sentiment Analysis with Vader\n",
    "- Word Embedding with BERT\n",
    "  \n",
    "A total of 7 distinctive features.\n",
    "\n",
    "The human readable features are: CountVectorizer because it is a count of the words throughout the interview, the Statistical Features, Part of Speech Tagging (with a little extra interpretation), and Sentiment Analysis with Vader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language feature selection part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Define the features and the target outcomes\n",
    "X = features[['word count', 'avg word length', 'pos tagging', 'sentiment', 'word embeddings']]\n",
    "y = outcomes  # Assuming 'outcomes' is a predefined variable containing the target outcomes\n",
    "\n",
    "# Apply SelectKBest to extract the top k features\n",
    "k = 5  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = X.columns[selector.get_support(indices=True)]\n",
    "\n",
    "# Display the selected features and their scores\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "for feature, score in zip(selected_features, feature_scores):\n",
    "    print(f\"Feature: {feature}, Score: {score}\")\n",
    "\n",
    "# Discuss findings\n",
    "# The selected features are the most relevant to the considered outcomes based on the ANOVA F-value.\n",
    "# These features can provide actionable insights to the user by highlighting the most important aspects\n",
    "# of the language used in the interviews. For example, if 'word count' is positively associated with\n",
    "# successful outcomes, users can focus on increasing the length of their responses. Similarly, if\n",
    "# 'sentiment' is negatively associated with successful outcomes, users can work on improving the\n",
    "# positivity of their language.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
