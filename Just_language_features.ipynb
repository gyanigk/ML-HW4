{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocessing function for transcripts\n",
    "def preprocess_text(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return \"\"  # Return an empty string if text is None\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    # # Removing Punctuation\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Removing Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)  # Return the processed text as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured DataFrame:\n",
      "  Participant participant&question  \\\n",
      "0          p1                 p1q1   \n",
      "1          p1                 p1q2   \n",
      "2          p1                 p1q3   \n",
      "3          p1                 p1q4   \n",
      "4          p1                 p1q5   \n",
      "\n",
      "                                            question  \\\n",
      "0                              So how are you doing?   \n",
      "1         Ok well  so please tell me about yourself.   \n",
      "2                                              mhhmm   \n",
      "3  So please tell me about a time that you demons...   \n",
      "4  Tell me about a time when your working on a te...   \n",
      "\n",
      "                                              answer  \n",
      "0                                    Im pretty good.  \n",
      "1  ok  uhm  so have you looked at my resume or sh...  \n",
      "2  So ah  my interest kinda laid both in a little...  \n",
      "3  Ok  uhm  one of the things we have to do for C...  \n",
      "4  Ahh  I guess the easiest team project I just I...  \n",
      "  Participant participant&question  \\\n",
      "0          p1                 p1q1   \n",
      "1          p1                 p1q2   \n",
      "2          p1                 p1q3   \n",
      "3          p1                 p1q4   \n",
      "4          p1                 p1q5   \n",
      "\n",
      "                                            question  \\\n",
      "0                              So how are you doing?   \n",
      "1         Ok well  so please tell me about yourself.   \n",
      "2                                              mhhmm   \n",
      "3  So please tell me about a time that you demons...   \n",
      "4  Tell me about a time when your working on a te...   \n",
      "\n",
      "                                              answer  \\\n",
      "0                                    Im pretty good.   \n",
      "1  ok  uhm  so have you looked at my resume or sh...   \n",
      "2  So ah  my interest kinda laid both in a little...   \n",
      "3  Ok  uhm  one of the things we have to do for C...   \n",
      "4  Ahh  I guess the easiest team project I just I...   \n",
      "\n",
      "                                   processed_answers   Overall  Excited  \n",
      "0                                    im pretti good.  5.297316  5.04389  \n",
      "1  ok uhm look resum i alright i guess ah i cours...  5.297316  5.04389  \n",
      "2  so ah interest kinda laid littl bit health car...  5.297316  5.04389  \n",
      "3  ok uhm one thing camp kesem orgin fundrais mon...  5.297316  5.04389  \n",
      "4  ahh i guess easiest team project i i last seme...  5.297316  5.04389  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Transcripts\n",
    "scores_file = \"data/scores.csv\"  # target\n",
    "transcripts_file = 'data/transcripts.csv'  # Replace with your transcripts file path\n",
    "scores_df = pd.read_csv(scores_file)\n",
    "transcripts_df = pd.read_csv(transcripts_file, header=None)\n",
    "\n",
    "# Step 2: Initialize a list to hold structured data\n",
    "structured_data = []\n",
    "\n",
    "# Step 3: Process each row in the DataFrame\n",
    "for index, row in transcripts_df.iterrows():\n",
    "    # participant_id = row[0]  # Extract participant ID from the first column using iloc\n",
    "    # transcript = row[1]  # Get the transcript from the second column using iloc\n",
    "    participant_id = row[0]\n",
    "    transcript = row[1]\n",
    "    parts = transcript.split('|')  # Split the transcript based on '|'\n",
    "    \n",
    "    # Initialize variables to hold questions and answers\n",
    "    current_question = \"\"\n",
    "    current_answer = \"\"\n",
    "    question_number = 0  # Initialize question number for the participant\n",
    "    \n",
    "    # Step 4: Iterate through the parts and extract questions and answers\n",
    "    for part in parts:\n",
    "        part = part.strip()  # Remove leading/trailing whitespace\n",
    "        if part.startswith(\"Interviewer:\"):\n",
    "            # If there's an ongoing answer, save it before starting a new question\n",
    "            if current_answer:\n",
    "                structured_data.append({\n",
    "                    'Participant': participant_id,  # Create unique question ID\n",
    "                    'participant&question': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "                    'question': current_question,\n",
    "                    'answer': current_answer.strip()\n",
    "                })\n",
    "                current_answer = \"\"  # Reset current answer\n",
    "            # Set the current question and increment question number\n",
    "            current_question = part.replace(\"Interviewer:\", \"\").strip()\n",
    "            question_number += 1  # Increment question number\n",
    "        elif part.startswith(\"Interviewee:\"):\n",
    "            # Append the answer\n",
    "            current_answer += part.replace(\"Interviewee:\", \"\").strip() + \" \"\n",
    "    \n",
    "    # After the loop, check if there's an ongoing answer to save\n",
    "    if current_question and current_answer:\n",
    "        structured_data.append({\n",
    "            'Participant': participant_id,  # Create unique question ID\n",
    "            'participant&question': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "            'question': current_question,\n",
    "            'answer': current_answer.strip()\n",
    "        })\n",
    "\n",
    "# Step 5: Create a DataFrame from the structured data\n",
    "structured_df = pd.DataFrame(structured_data)\n",
    "\n",
    "# Step 6: Display the structured DataFrame\n",
    "print(\"Structured DataFrame:\")\n",
    "print(structured_df.head())\n",
    "# Apply preprocessing to the transcript data\n",
    "structured_df['processed_answers'] = structured_df['answer'].apply(preprocess_text)\n",
    "# Create the merged df for directly for later use\n",
    "merged_df = structured_df.merge(scores_df[['Participant', 'Overall', 'Excited']], on='Participant', how='left')\n",
    "# Save to a new CSV file\n",
    "merged_df.to_csv('data/clean_transcripts.csv', index=False)  \n",
    "print(merged_df.head())\n",
    "# Step 7: Save the Structured Data to a CSV file\n",
    "# structured_df.to_csv('data/clean_transcripts.csv', index=False)  # Save to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(structured_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the transcript data\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m structured_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_answers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m structured_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Create the merged df for directly for later use\u001b[39;00m\n\u001b[0;32m     64\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m structured_df\u001b[38;5;241m.\u001b[39mmerge(scores_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticipant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExcited\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticipant\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": [
    "scores_file = \"data/scores.csv\" # target\n",
    "transcript_file = \"data/transcripts.csv\" # inpput data\n",
    "scores_data = pd.read_csv(scores_file)\n",
    "transcript_data = pd.read_csv(transcript_file)\n",
    "# Make the entire transcript lower case\n",
    "transcript_data['transcript'] = transcript_data['transcript'].str.lower()\n",
    "\n",
    "# drop the unnecesary columns \n",
    "columns_to_drop = ['Answer', 'Question']\n",
    "transcript_data = transcript_data.drop(columns=[col for col in columns_to_drop if col in transcript_data.columns])\n",
    "\n",
    "#creating structured dataset for improving regression based scoring; since the scoring is based on question wise\n",
    "structured_data = []\n",
    "# Iterate through each row in the transcripts DataFrame\n",
    "for index, row in transcript_data.iterrows():\n",
    "    transcript = row[1]  # Get the transcript from the first column\n",
    "    participant_id = row[0]  # Extract participant ID from the transcript\n",
    "    # print(f\"Raw Transcript for {participant_id}: {transcript}\")  # Debugging: Print the raw transcript\n",
    "    parts = transcript.split('|')  # Split the transcript based on '|'\n",
    "    # Print the split parts for debugging\n",
    "    # print(f\"Split Parts for {participant_id}: {parts}\")  # Debugging: Print the split parts\n",
    "    # Initialize variables to hold questions and answers\n",
    "    current_question = \"\"\n",
    "    current_answer = \"\"\n",
    "    question_number = 0  # Initialize question number for the participant\n",
    "    # Step 4: Iterate through the parts and extract questions and answers\n",
    "    for part in parts:\n",
    "        part = part.strip()  # Remove leading/trailing whitespace\n",
    "        if part.startswith(\"Interviewer:\"):\n",
    "            # If there's an ongoing answer, save it before starting a new question\n",
    "            if current_answer:\n",
    "                structured_data.append({\n",
    "                    'participant_id': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "                    'question': current_question,\n",
    "                    'answer': current_answer.strip()\n",
    "                })\n",
    "                current_answer = \"\"  # Reset current answer\n",
    "            \n",
    "            # Set the current question and increment question number\n",
    "            current_question = part.replace(\"Interviewer:\", \"\").strip()\n",
    "            question_number += 1  # Increment question number\n",
    "        elif part.startswith(\"Interviewee:\"):\n",
    "            # Append the answer\n",
    "            current_answer += part.replace(\"Interviewee:\", \"\").strip() + \" \"\n",
    "    \n",
    "    # After the loop, check if there's an ongoing answer to save\n",
    "    if current_question and current_answer:\n",
    "        structured_data.append({\n",
    "            'participant_id': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "            'question': current_question,\n",
    "            'answer': current_answer.strip()\n",
    "        })\n",
    "# Convert structured data to a DataFrame for better visualization (optional)\n",
    "structured_df = pd.DataFrame(structured_data)\n",
    "\n",
    "# Apply preprocessing to the transcript data\n",
    "structured_df['processed_answers'] = structured_df['answer'].apply(preprocess_text)\n",
    "#create the merged df for directly for later use\n",
    "merged_df = structured_df.merge(scores_data[['Participant', 'Overall', 'Excited']], on='Participant', how='left')\n",
    "# print(merged_df.head())\n",
    "merged_df.to_csv('data/clean_transcripts.csv', index=False)  # Save to a new CSV file\n",
    "print(merged_df.head())\n",
    "# transcript_data[\"Overall\"] = scores_data[\"Overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the transcripts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Participant                                         transcript   Overall  \\\n",
      "0          p1  interviewer: so how are you doing?|interviewee...  5.297316   \n",
      "1         p10  interviewer: so  how you doing?|interviewee: g...  4.725115   \n",
      "2         p11  interviewer: so  tell me about yourself. |inte...  5.010430   \n",
      "3         p12  interviewer: so how are you doing today?|inter...  5.038526   \n",
      "4         p13  interviewer: how are you doing today?|intervie...  4.251251   \n",
      "\n",
      "    Excited                               processed_transcript  \n",
      "0  5.043890  interviewer: doing?|interviewee: im pretti goo...  \n",
      "1  4.383947  interviewer: doing?|interviewee: great you?|in...  \n",
      "2  4.297760  interviewer: tell yourself. |interviewee: uhh ...  \n",
      "3  5.322526  interviewer: today?|interviewee: i'm good you?...  \n",
      "4  3.579510  interviewer: today?|interviewee: good.|intervi...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we use cleaning the stop words and stemmer for improving the textual data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: ['11', '89', '65', '42', '57', '29', '17', '7', '1', '10', '67', '47', '49', '81']\n",
      "\tLength:  14\n",
      "Fold 2: ['24', '16', '52', '69', '12', '21', '73', '53', '37', '44', '78', '64', '3', '31']\n",
      "\tLength:  14\n",
      "Fold 3: ['6', '86', '50', '45', '13', '20', '74', '58', '60', '85', '70', '56', '30', '8']\n",
      "\tLength:  14\n",
      "Fold 4: ['72', '15', '33', '32', '27', '25', '43', '79', '62', '61', '71', '84', '34', '4']\n",
      "\tLength:  14\n",
      "Fold 5: ['5', '48', '22', '59', '55', '66', '76', '83', '14', '80', '35', '77', '63']\n",
      "\tLength:  13\n"
     ]
    }
   ],
   "source": [
    "# Splitting the participants into 5 folds\n",
    "num_folds = 5\n",
    "\n",
    "# Grab participant numbers from the scores csv file\n",
    "interviews = scores_data['Participant'].unique()\n",
    "participants = list(set([re.sub(r'^pp?|q\\d+', '', item) for item in interviews]))\n",
    "random.shuffle(participants)\n",
    "participant_folds = [participants[i::num_folds] for i in range(num_folds)]\n",
    "\n",
    "for i, fold in enumerate(participant_folds):\n",
    "  print(f\"Fold {i + 1}: {fold}\")\n",
    "  print(\"\\tLength: \", len(fold))\n",
    "\n",
    "# Grab all the correct interview names associated with each participant\n",
    "interview_folds = []\n",
    "for fold in participant_folds:\n",
    "  interview_folds.append([item for num in fold for item in (f\"p{num}\", f\"pp{num}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data, fold_number):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets based on a specified fold.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The complete dataset.\n",
    "        fold_number (int): The fold to use for testing (0-based index).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_set, validation_set, testing_set)\n",
    "    \"\"\"\n",
    "    # Quick check on fold number\n",
    "    assert 0 <= fold_number < len(interview_folds), \"Fold_number must be between 0 and len(folds) - 1\"\n",
    "\n",
    "    # Split the data\n",
    "    test_set = data[data['Participant'].isin( interview_folds[fold_number] )]\n",
    "    val_set = data[data['Participant'].isin( interview_folds[(fold_number + 1) % len(interview_folds)] )]\n",
    "    train_set_parts = [item for i, fold in enumerate(interview_folds) if i not in [fold_number, (fold_number + 1) % len(interview_folds)] for item in fold]\n",
    "    train_set = data[data['Participant'].isin(train_set_parts)]\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')  # POS Tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon') # Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
