{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocessing function for transcripts\n",
    "def preprocess_text(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return \"\"  # Return an empty string if text is None\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    # # Removing Punctuation\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Removing Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)  # Return the processed text as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured DataFrame:\n",
      "  Participant participant&question  \\\n",
      "0          p1                 p1q1   \n",
      "1          p1                 p1q2   \n",
      "2          p1                 p1q3   \n",
      "3          p1                 p1q4   \n",
      "4          p1                 p1q5   \n",
      "\n",
      "                                            question  \\\n",
      "0                              So how are you doing?   \n",
      "1         Ok well  so please tell me about yourself.   \n",
      "2                                              mhhmm   \n",
      "3  So please tell me about a time that you demons...   \n",
      "4  Tell me about a time when your working on a te...   \n",
      "\n",
      "                                              answer  \n",
      "0                                    Im pretty good.  \n",
      "1  ok  uhm  so have you looked at my resume or sh...  \n",
      "2  So ah  my interest kinda laid both in a little...  \n",
      "3  Ok  uhm  one of the things we have to do for C...  \n",
      "4  Ahh  I guess the easiest team project I just I...  \n",
      "  Participant participant&question  \\\n",
      "0          p1                 p1q1   \n",
      "1          p1                 p1q2   \n",
      "2          p1                 p1q3   \n",
      "3          p1                 p1q4   \n",
      "4          p1                 p1q5   \n",
      "\n",
      "                                            question  \\\n",
      "0                              So how are you doing?   \n",
      "1         Ok well  so please tell me about yourself.   \n",
      "2                                              mhhmm   \n",
      "3  So please tell me about a time that you demons...   \n",
      "4  Tell me about a time when your working on a te...   \n",
      "\n",
      "                                              answer  \\\n",
      "0                                    Im pretty good.   \n",
      "1  ok  uhm  so have you looked at my resume or sh...   \n",
      "2  So ah  my interest kinda laid both in a little...   \n",
      "3  Ok  uhm  one of the things we have to do for C...   \n",
      "4  Ahh  I guess the easiest team project I just I...   \n",
      "\n",
      "                                   processed_answers   Overall  Excited  \n",
      "0                                    im pretti good.  5.297316  5.04389  \n",
      "1  ok uhm look resum i alright i guess ah i cours...  5.297316  5.04389  \n",
      "2  so ah interest kinda laid littl bit health car...  5.297316  5.04389  \n",
      "3  ok uhm one thing camp kesem orgin fundrais mon...  5.297316  5.04389  \n",
      "4  ahh i guess easiest team project i i last seme...  5.297316  5.04389  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Transcripts\n",
    "scores_file = \"data/scores.csv\"  # target\n",
    "scores_df = pd.read_csv(scores_file)\n",
    "\n",
    "transcripts_file = 'data/transcripts.csv'  # Replace with your transcripts file path\n",
    "transcripts_df = pd.read_csv(transcripts_file, header=None)\n",
    "\n",
    "\n",
    "# Step 2: Initialize a list to hold structured data\n",
    "structured_data = []\n",
    "\n",
    "# Step 3: Process each row in the DataFrame\n",
    "for index, row in transcripts_df.iterrows():\n",
    "    # participant_id = row[0]  # Extract participant ID from the first column using iloc\n",
    "    # transcript = row[1]  # Get the transcript from the second column using iloc\n",
    "    participant_id = row[0]\n",
    "    transcript = row[1]\n",
    "    parts = transcript.split('|')  # Split the transcript based on '|'\n",
    "    \n",
    "    # Initialize variables to hold questions and answers\n",
    "    current_question = \"\"\n",
    "    current_answer = \"\"\n",
    "    question_number = 0  # Initialize question number for the participant\n",
    "    \n",
    "    # Step 4: Iterate through the parts and extract questions and answers\n",
    "    for part in parts:\n",
    "        part = part.strip()  # Remove leading/trailing whitespace\n",
    "        if part.startswith(\"Interviewer:\"):\n",
    "            # If there's an ongoing answer, save it before starting a new question\n",
    "            if current_answer:\n",
    "                structured_data.append({\n",
    "                    'Participant': participant_id,  # Create unique question ID\n",
    "                    'participant&question': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "                    'question': current_question,\n",
    "                    'answer': current_answer.strip()\n",
    "                })\n",
    "                current_answer = \"\"  # Reset current answer\n",
    "            # Set the current question and increment question number\n",
    "            current_question = part.replace(\"Interviewer:\", \"\").strip()\n",
    "            question_number += 1  # Increment question number\n",
    "        elif part.startswith(\"Interviewee:\"):\n",
    "            # Append the answer\n",
    "            current_answer += part.replace(\"Interviewee:\", \"\").strip() + \" \"\n",
    "    \n",
    "    # After the loop, check if there's an ongoing answer to save\n",
    "    if current_question and current_answer:\n",
    "        structured_data.append({\n",
    "            'Participant': participant_id,  # Create unique question ID\n",
    "            'participant&question': f\"{participant_id}q{question_number}\",  # Create unique question ID\n",
    "            'question': current_question,\n",
    "            'answer': current_answer.strip()\n",
    "        })\n",
    "\n",
    "# Step 5: Create a DataFrame from the structured data\n",
    "structured_df = pd.DataFrame(structured_data)\n",
    "\n",
    "# Step 6: Display the structured DataFrame\n",
    "print(\"Structured DataFrame:\")\n",
    "print(structured_df.head())\n",
    "# Apply preprocessing to the transcript data; # we use cleaning the stop words and stemmer for improving the textual data\n",
    "structured_df['processed_answers'] = structured_df['answer'].apply(preprocess_text)\n",
    "# Create the merged df for directly for later use\n",
    "merged_df = structured_df.merge(scores_df[['Participant', 'Overall', 'Excited']], on='Participant', how='left')\n",
    "# Save to a new CSV file\n",
    "merged_df.to_csv('data/clean_transcripts.csv', index=False)  \n",
    "print(merged_df.head())\n",
    "# Step 7: Save the Structured Data to a CSV file\n",
    "# structured_df.to_csv('data/clean_transcripts.csv', index=False)  # Save to a new CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the transcripts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: ['47', '72', '67', '85', '69', '62', '65', '6', '45', '70', '76', '44', '34', '30']\n",
      "\tLength:  14\n",
      "Fold 2: ['50', '22', '29', '8', '14', '81', '1', '17', '55', '37', '43', '59', '57', '12']\n",
      "\tLength:  14\n",
      "Fold 3: ['89', '71', '21', '53', '74', '7', '86', '56', '61', '73', '16', '79', '42', '63']\n",
      "\tLength:  14\n",
      "Fold 4: ['77', '33', '80', '24', '5', '10', '52', '25', '66', '3', '15', '27', '4', '84']\n",
      "\tLength:  14\n",
      "Fold 5: ['83', '48', '60', '32', '11', '64', '13', '78', '49', '20', '58', '31', '35']\n",
      "\tLength:  13\n"
     ]
    }
   ],
   "source": [
    "# Splitting the participants into 5 folds\n",
    "num_folds = 5\n",
    "\n",
    "# Grab participant numbers from the scores csv file\n",
    "interviews = scores_df['Participant'].unique()\n",
    "participants = list(set([re.sub(r'^pp?|q\\d+', '', item) for item in interviews]))\n",
    "random.shuffle(participants)\n",
    "participant_folds = [participants[i::num_folds] for i in range(num_folds)]\n",
    "\n",
    "for i, fold in enumerate(participant_folds):\n",
    "  print(f\"Fold {i + 1}: {fold}\")\n",
    "  print(\"\\tLength: \", len(fold))\n",
    "\n",
    "# Grab all the correct interview names associated with each participant\n",
    "interview_folds = []\n",
    "for fold in participant_folds:\n",
    "  interview_folds.append([item for num in fold for item in (f\"p{num}\", f\"pp{num}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data, fold_number):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets based on a specified fold.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The complete dataset.\n",
    "        fold_number (int): The fold to use for testing (0-based index).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_set, validation_set, testing_set)\n",
    "    \"\"\"\n",
    "    # Quick check on fold number\n",
    "    assert 0 <= fold_number < len(interview_folds), \"Fold_number must be between 0 and len(folds) - 1\"\n",
    "\n",
    "    # Split the data\n",
    "    test_set = data[data['Participant'].isin( interview_folds[fold_number] )]\n",
    "    val_set = data[data['Participant'].isin( interview_folds[(fold_number + 1) % len(interview_folds)] )]\n",
    "    train_set_parts = [item for i, fold in enumerate(interview_folds) if i not in [fold_number, (fold_number + 1) % len(interview_folds)] for item in fold]\n",
    "    train_set = data[data['Participant'].isin(train_set_parts)]\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')  # POS Tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon') # Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
